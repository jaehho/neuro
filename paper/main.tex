\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage[style=numeric-comp, backend=biber, sorting=none]{biblatex}

% Layout settings for readability
\geometry{a4paper, margin=1in}
\linespread{1.15}

% Bibliography resource
\addbibresource{references.bib}

% Theorem/Definition styles
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

\section{Microscopic Foundation: Neural Dynamics}

Before deriving the macroscopic plasticity rules, we must first establish the microscopic dynamics of the individual neurons comprising the network. We employ the Leaky Integrate-and-Fire (LIF) model, a standard reduction of the Hodgkin-Huxley formalism that captures the essential sub-threshold integration and thresholding behavior of cortical neurons \parencite{gerstner_neuronal_2014}.

\subsection{Membrane Potential Dynamics}
The state of a postsynaptic neuron $i$ is described by its membrane potential $V_i(t)$. In the absence of input, the membrane potential relaxes to a resting potential $E_L$. The evolution of $V_i(t)$ is governed by the conservation of current across the cell membrane, modeled as an RC circuit consisting of a leakage resistor $R_m$ and a membrane capacitor $C_m$ in parallel \parencite{dayan_theoretical_2001}:



\begin{equation} \label{eq:lif_voltage}
    \tau_m \frac{dV_i(t)}{dt} = -(V_i(t) - E_L) + R_m I_{syn, i}(t) + R_m I_{ext, i}(t),
\end{equation}
where $\tau_m = R_m C_m$ represents the membrane time constant. In cortical pyramidal neurons, $\tau_m$ typically lies in the range of 10--20 ms \parencite{dayan_theoretical_2001}. The term $I_{syn, i}(t)$ represents the total synaptic current received from presynaptic neurons, and $I_{ext, i}(t)$ accounts for any external background currents or noise.

\subsection{Synaptic Interaction}
The synaptic current $I_{syn, i}(t)$ is determined by the activity of the presynaptic population. Let the spike train of a presynaptic neuron $j$ be denoted by $\rho_j(t) = \sum_k \delta(t - t_j^k)$. The specific mathematical definition of these spike times is provided in Section \ref{sec:spike_gen}.

The arrival of a spike from neuron $j$ induces a transient change in the input to neuron $i$. We utilize the \textit{current-based} approximation, which assumes that synaptic currents are independent of the postsynaptic membrane potential (unlike conductance-based models where $I \propto g(t)(V - E_{syn})$). This approximation is standard for analyzing network-level learning dynamics \parencite{gerstner_neuronal_2014}. The total synaptic current is the linear sum of filtered presynaptic spikes, weighted by the synaptic efficacy $w_{ij}$ (the dynamics of which are derived in Section \ref{sec:weight_constraints}):

\begin{equation} \label{eq:synaptic_current}
    I_{syn, i}(t) = \sum_{j} w_{ij} \int_{-\infty}^{t} \alpha(t - s) \rho_j(s) \, ds,
\end{equation}
where $\alpha(t)$ is the postsynaptic current (PSC) kernel. Following \textcite{dayan_theoretical_2001}, this is modeled as an exponential decay function $\alpha(t) = \frac{1}{\tau_s} e^{-t/\tau_s} \Theta(t)$, where $\Theta(t)$ is the Heaviside step function. The synaptic time constant $\tau_s$ typically ranges from 2--10 ms for fast AMPA/GABA receptors \parencite{destexhe_kinetic_1998}.

\subsection{Spike Generation Mechanism} \label{sec:spike_gen}
The continuous voltage dynamics defined above give rise to discrete events. Mathematically, a spike is not a duration but a single point in time $t_i^k$. This is defined as the moment the membrane potential crosses a fixed threshold $\vartheta$ from below:
\begin{equation} \label{eq:threshold}
    t_i^k : V_i(t_i^k) = \vartheta \quad \text{and} \quad \frac{dV_i}{dt}\bigg|_{t=t_i^k} > 0.
\end{equation}
While a simple condition $V > \vartheta$ is often used in discrete-time simulations, the derivative condition $\frac{dV}{dt} > 0$ is required analytically to distinguish the leading edge of the spike (the event) from the subsequent duration of the pulse or the repolarization phase.



Following the formalism in \textcite{dayan_theoretical_2001}, the absolute refractory period is modeled by interrupting the integration of \eqref{eq:lif_voltage}. When $V_i$ reaches threshold at $t_i^k$, the potential is reset to $V_{reset} < \vartheta$. The dynamics in \eqref{eq:lif_voltage} are then \textit{suspended} for the duration $\tau_{ref}$ (typically 2--5 ms). Integration resumes at $t = t_i^k + \tau_{ref}$ with the initial condition $V_i(t_i^k + \tau_{ref}) = V_{reset}$. This mechanism defines the postsynaptic spike train $\rho_i(t) = \sum_k \delta(t - t_i^k)$, which serves as the input to the plasticity equations in the following section.

\section{Mathematical Formulation of the Three-Factor Plasticity Model}

Having defined the generation of spike times via the LIF dynamics, we now analyze the evolution of the synaptic weights $w_{ij}$. We focus on a plasticity rule belonging to the class of \textit{three-factor learning rules}, as reviewed by \textcite{fremaux_neuromodulated_2016}. 

In standard STDP, weight changes depend on two local factors:
\begin{itemize}
    \item \textbf{Factor 1:} Presynaptic spike timing ($t_j^k$).
    \item \textbf{Factor 2:} Postsynaptic spike timing ($t_i^k$).
\end{itemize}
In the three-factor framework, these local correlations are gated by a global third signal, \textbf{Factor 3}, representing neuromodulation or reward.



\subsection{Neural Activity and Notation}
Following the framework established in the previous section, the neural response functions are formally treated as sums of Dirac distributions:
\begin{equation}
    \rho_j(t) = \sum_{k=1}^{N_j} \delta(t - t_j^k) \quad \text{and} \quad \rho_i(t) = \sum_{k=1}^{N_i} \delta(t - t_i^k).
\end{equation}
Here, $N_j$ and $N_i$ denote the total number of spikes fired by each neuron over the simulation interval $t \in [0, T]$, where $T$ represents the total duration of the learning epoch. These counts are determined by the dynamics in \eqref{eq:lif_voltage} and \eqref{eq:threshold}. Note that while the equations are deterministic, the spike trains often exhibit Poisson-like variability due to stochastic external currents $I_{ext}(t)$ \parencite{dayan_theoretical_2001}.

\subsection{Local Dynamics: The Eligibility Trace}
A central feature of this model is that the coincidence of spikes creates a temporary memory trace, $E_{ij}(t)$, known as the \textit{eligibility trace} \parencite{fremaux_neuromodulated_2016}. This trace allows the synapse to bridge the temporal delay between millisecond-scale neural activity and delayed reward signals. The eligibility trace evolves according to:
\begin{equation} \label{eq:eligibility}
    \tau_e \frac{dE_{ij}(t)}{dt} = -E_{ij}(t) + S_{ij}(t),
\end{equation}
where $\tau_e$ is the decay time constant. For reinforcement learning tasks, $\tau_e$ is typically on the order of hundreds of milliseconds to seconds (e.g., 0.1--1.0 s), significantly longer than the membrane time constant $\tau_m$ \parencite{gerstner_neuronal_2014}.

The driving term $S_{ij}(t)$ represents the instantaneous induction of Spike-Timing-Dependent Plasticity (STDP). To define $S_{ij}(t)$, we use variables $x_j(t)$ and $y_i(t)$ that track the recent history of presynaptic and postsynaptic activity:
\begin{align}
    \tau_+ \frac{dx_j(t)}{dt} &= -x_j(t) + \rho_j(t), \\
    \tau_- \frac{dy_i(t)}{dt} &= -y_i(t) + \rho_i(t),
\end{align}
where $\tau_+$ and $\tau_-$ are the time constants for the potentiation and depression windows. Experimental measurements suggest these values are typically in the range of 20--40 ms \parencite{bi_synaptic_1998}.

The STDP induction term $S_{ij}(t)$ combines Long-Term Potentiation (LTP) and Long-Term Depression (LTD):
\begin{equation}
    S_{ij}(t) = \underbrace{A_+(w_{ij}) x_j(t) \rho_i(t)}_{\text{LTP contribution}} - \underbrace{A_-(w_{ij}) y_i(t) \rho_j(t)}_{\text{LTD contribution}}.
\end{equation}

\subsection{Global Dynamics: Neuromodulated Update}
The actual change in synaptic weight is governed by a global neuromodulatory signal $M(t)$:
\begin{equation}
    \frac{dw_{ij}(t)}{dt} = M(t) E_{ij}(t).
\end{equation}
The signal $M(t)$ is modeled as a Reward Prediction Error (RPE), calculated as the difference between the instantaneous reward $R(t)$ and a baseline expectation $\bar{R}(t)$:
\begin{equation}
    M(t) = R(t) - \bar{R}(t).
\end{equation}
Here, $\bar{R}(t)$ serves as a reference point, allowing for bidirectional regulation of synaptic weights \parencite{fremaux_neuromodulated_2016}.

\subsection{Weight Constraints and Stability} \label{sec:weight_constraints}
The synaptic weight $w_{ij}$ represents the efficacy of the connection from neuron $j$ to neuron $i$. To prevent unbounded growth during the learning process, we constrain the weight to a closed interval $w_{ij} \in [0, w_{\text{max}}]$. The parameter $w_{\text{max}}$ represents the physiological saturation limit of synaptic efficacy.

We enforce these bounds using "soft bound" dependencies in the scaling functions:
\begin{equation}
    A_+(w_{ij}) = \eta_+ (w_{\text{max}} - w_{ij}) \quad \text{and} \quad A_-(w_{ij}) = \eta_- w_{ij},
\end{equation}
where $\eta_+, \eta_-$ are the learning rates. This linear dependence ensures that the rate of weight change drops to zero as the weight approaches either limit ($0$ or $w_{\text{max}}$), naturally stabilizing the dynamics \parencite{gerstner_neuronal_2014}.

\printbibliography

\end{document}

@article{abbottBuildingFunctionalNetworks2016,
  title = {Building Functional Networks of Spiking Model Neurons},
  author = {Abbott, L F and DePasquale, Brian and Memmesheimer, Raoul-Martin},
  year = 2016,
  month = mar,
  journal = {Nature Neuroscience},
  volume = {19},
  number = {3},
  pages = {350--355},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4241},
  urldate = {2025-11-20},
  langid = {english},
  file = {C:\Users\jaeho\Zotero\storage\TLVJ4DQN\AbbottNatNeuro16.pdf}
}

@article{biSynapticModificationsCultured1998,
  title = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}: {{Dependence}} on {{Spike Timing}}, {{Synaptic Strength}}, and {{Postsynaptic Cell Type}}},
  shorttitle = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  year = 1998,
  month = dec,
  journal = {The Journal of Neuroscience},
  volume = {18},
  number = {24},
  pages = {10464--10472},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.18-24-10464.1998},
  urldate = {2026-02-18},
  abstract = {In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation (LTP), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression (LTD). Significant LTP occurred only at synapses with relatively low initial strength, whereas the extent of LTD did not show obvious dependence on the initial synaptic strength. Both LTP and LTD depended on the activation of NMDA receptors and were absent in cases in which the postsynaptic neurons were GABAergic in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of LTD and reduced the extent of LTP. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb's rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {C:\Users\jaeho\Zotero\storage\BPWPGPTM\Bi and Poo - 1998 - Synaptic Modifications in Cultured Hippocampal Neurons Dependence on Spike Timing, Synaptic Strengt.pdf}
}

@article{caiModelReductionCaptures2021,
  title = {Model {{Reduction Captures Stochastic Gamma Oscillations}} on {{Low-Dimensional Manifolds}}},
  author = {Cai, Yuhang and Wu, Tianyi and Tao, Louis and Xiao, Zhuo-Cheng},
  year = 2021,
  month = aug,
  journal = {Frontiers in Computational Neuroscience},
  volume = {15},
  publisher = {Frontiers},
  issn = {1662-5188},
  doi = {10.3389/fncom.2021.678688},
  urldate = {2025-12-15},
  abstract = {Gamma frequency oscillations (25-140 Hz), observed in the neural activities within many brain regions, have long been regarded as a physiological basis underlying many brain functions, such as memory and attention. Among numerous theoretical and computational modeling studies, gamma oscillations have been found in biologically realistic spiking network models of the primary visual cortex. However, due to its high dimensionality and strong nonlinearity, it is generally difficult to perform detailed theoretical analysis of the emergent gamma dynamics. Here we propose a suite of Markovian model reduction methods with varying levels of complexity and apply it to spiking network models exhibiting heterogeneous dynamical regimes, ranging from nearly homogeneous firing to strong synchrony in the gamma band. The reduced models not only successfully reproduce gamma oscillations in the full model, but also exhibit the same dynamical features as we vary parameters. Most remarkably, the invariant measure of the coarse-grained Markov process reveals a two-dimensional surface in state space upon which the gamma dynamics mainly resides. Our results suggest that the statistical features of gamma oscillations strongly depend on the subthreshold neuronal distributions. Because of the generality of the Markovian assumptions, our dimensional reduction methods offer a powerful toolbox for theoretical examinations of other complex cortical spatio-temporal behaviors observed in both neurophysiological experiments and numerical simulations.},
  langid = {english},
  keywords = {Coarse-graining method,gamma oscillations,homogeneity,Model reduction algorithm,reduction,synchrony},
  file = {C:\Users\jaeho\Zotero\storage\RBFJXIAS\Cai et al. - 2021 - Model Reduction Captures Stochastic Gamma Oscillations on Low-Dimensional Manifolds.pdf}
}

@article{daviesSupervisedLearningSpatial2025,
  title = {Supervised Learning of Spatial Features with {{STDP}} and Homeostasis Using {{Spiking Neural Networks}} on {{SpiNNaker}}},
  author = {Davies, Sergio and Gait, Andrew and Rowley, Andrew and Di Nuovo, Alessandro},
  year = 2025,
  month = jan,
  journal = {Neurocomputing},
  volume = {611},
  pages = {128650},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2024.128650},
  urldate = {2025-12-01},
  abstract = {Artificial Neural Networks (ANN) have gained significant popularity thanks to their ability to learn using the well-known backpropagation algorithm. Conversely, Spiking Neural Networks (SNNs), despite having broader capabilities than ANNs, have always posed challenges in the training phase. This paper shows a new method to perform supervised learning on SNNs, using Spike Timing Dependent Plasticity (STDP) and homeostasis, aiming at training the network to identify spatial patterns. Spatial patterns refer to spike patterns without a time component, where all spike events occur simultaneously. The method is tested using the SpiNNaker digital architecture. A SNN is trained to recognise one or multiple patterns and performance metrics are extracted to measure the performance of the network. Some considerations are drawn from the results showing that, in the case of a single trained pattern, the network behaves as the ideal detector, with 100\% accuracy in detecting the trained pattern. However, as the number of trained patterns on a single network increases, the accuracy of identification is linked to the similarities between these patterns. This method of training an SNN to detect spatial patterns may be applied to pattern recognition in static images or traffic analysis in computer networks, where each network packet represents a spatial pattern. It will be stipulated that the homeostatic factor may enable the network to detect patterns with some degree of similarity, rather than only perfectly matching patterns. The principles outlined in this article serve as the fundamental building blocks for more complex systems that utilise both spatial and temporal patterns by converting specific features of input signals into spikes. One example of such a system is a computer network packet classifier, tasked with real-time identification of packet streams based on features within the packet content.},
  keywords = {Important,SNN,Spatial pattern,Spike Timing Dependent Plasticity,Spiking Neural Networks,STDP,Supervised learning},
  file = {C\:\\Users\\jaeho\\Zotero\\storage\\P886E4M5\\Davies et al. - 2025 - Supervised learning of spatial features with STDP and homeostasis using Spiking Neural Networks on S.pdf;C\:\\Users\\jaeho\\Zotero\\storage\\Y8HF87J7\\Davies et al. - 2025 - Supervised learning of spatial features with STDP and homeostasis using Spiking Neural Networks on S.pdf;C\:\\Users\\jaeho\\Zotero\\storage\\35J8ZLE6\\S0925231224014218.html}
}

@book{dayanTheoreticalNeuroscienceComputational2001,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, Laurence F.},
  year = 2001,
  series = {Computational Neuroscience},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-04199-7 978-0-262-54185-5},
  langid = {english},
  keywords = {Important},
  file = {C:\Users\jaeho\Zotero\storage\J42HQB98\Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathematical modeling of neural systems.pdf}
}

@article{destexheKineticModelsSynaptic,
  title = {Kinetic {{Models}} of {{Synaptic Transmission}}},
  author = {Destexhe, A and Mainen, Z F and Sejnowski, T J and Koch, C (EDITOR) and Segev, I (EDITOR)},
  langid = {english},
  file = {C:\Users\jaeho\Zotero\storage\FDFQW86C\Destexhe et al. - Kinetic Models of Synaptic Transmission.pdf}
}

@article{dongUnsupervisedSTDPbasedSpiking2023,
  title = {An Unsupervised {{STDP-based}} Spiking Neural Network Inspired by Biologically Plausible Learning Rules and Connections},
  author = {Dong, Yiting and Zhao, Dongcheng and Li, Yang and Zeng, Yi},
  year = 2023,
  month = aug,
  journal = {Neural Networks},
  volume = {165},
  pages = {799--808},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2023.06.019},
  urldate = {2025-11-20},
  abstract = {The backpropagation algorithm has promoted the rapid development of deep learning, but it relies on a large amount of labeled data and still has a large gap with how humans learn. The human brain can quickly learn various conceptual knowledge in a self-organized and unsupervised manner, accomplished through coordinating various learning rules and structures in the human brain. Spike-timing-dependent plasticity (STDP) is a general learning rule in the brain, but spiking neural networks (SNNs) trained with STDP alone is inefficient and perform poorly. In this paper, taking inspiration from short-term synaptic plasticity, we design an adaptive synaptic filter and introduce the adaptive spiking threshold as the neuron plasticity to enrich the representation ability of SNNs. We also introduce an adaptive lateral inhibitory connection to adjust the spikes balance dynamically to help the network learn richer features. To speed up and stabilize the training of unsupervised spiking neural networks, we design a samples temporal batch STDP (STB-STDP), which updates weights based on multiple samples and moments. By integrating the above three adaptive mechanisms and STB-STDP, our model greatly accelerates the training of unsupervised spiking neural networks and improves the performance of unsupervised SNNs on complex tasks. Our model achieves the current state-of-the-art performance of unsupervised STDP-based SNNs in the MNIST and FashionMNIST datasets. Further, we tested on the more complex CIFAR10 dataset, and the results fully illustrate the superiority of our algorithm. Our model is also the first work to apply unsupervised STDP-based SNNs to CIFAR10. At the same time, in the small-sample learning scenario, it will far exceed the supervised ANN using the same structure.},
  keywords = {Bio-Plausible,Brain inspired connection,MNIST,Plasticity learning rule,Spiking neural network,Unsupervised},
  file = {C\:\\Users\\jaeho\\Zotero\\storage\\Z7CLLKTB\\Dong et al. - 2023 - An unsupervised STDP-based spiking neural network inspired by biologically plausible learning rules.pdf;C\:\\Users\\jaeho\\Zotero\\storage\\48NQUP6T\\S0893608023003301.html}
}

@article{fremauxNeuromodulatedSpikeTimingDependentPlasticity2016,
  title = {Neuromodulated {{Spike-Timing-Dependent Plasticity}}, and {{Theory}} of {{Three-Factor Learning Rules}}},
  author = {Fr{\'e}maux, Nicolas and Gerstner, Wulfram},
  year = 2016,
  month = jan,
  journal = {Frontiers in Neural Circuits},
  volume = {9},
  pages = {85},
  issn = {1662-5110},
  doi = {10.3389/fncir.2015.00085},
  urldate = {2025-12-12},
  abstract = {Classical Hebbian learning puts the emphasis on joint pre- and postsynaptic activity, but neglects the potential role of neuromodulators. Since neuromodulators convey information about novelty or reward, the influence of neuromodulators on synaptic plasticity is useful not just for action learning in classical conditioning, but also to decide ``when'' to create new memories in response to a flow of sensory stimuli. In this review, we focus on timing requirements for pre- and postsynaptic activity in conjunction with one or several phasic neuromodulatory signals. While the emphasis of the text is on conceptual models and mathematical theories, we also discuss some experimental evidence for neuromodulation of Spike-Timing-Dependent Plasticity. We highlight the importance of synaptic mechanisms in bridging the temporal gap between sensory stimulation and neuromodulatory signals, and develop a framework for a class of neo-Hebbian three-factor learning rules that depend on presynaptic activity, postsynaptic variables as well as the influence of neuromodulators.},
  pmcid = {PMC4717313},
  pmid = {26834568},
  keywords = {Important},
  file = {C:\Users\jaeho\Zotero\storage\JCE8UNXE\Fr√©maux and Gerstner - 2016 - Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules.pdf}
}

@book{friedbergLinearAlgebra2003,
  title = {Linear Algebra},
  author = {Friedberg, Stephen H. and Insel, A. and Spence, Lawrence E.},
  year = 2003,
  edition = {4. ed., International ed},
  publisher = {Prentice-Hall, Pearson Education International},
  address = {Upper Saddle River, NJ},
  isbn = {978-0-13-008451-4 978-0-13-120266-5},
  langid = {english},
  file = {C:\Users\jaeho\Zotero\storage\8PXQLBR7\Friedberg et al. - 2003 - Linear algebra.pdf}
}

@article{frostHighefficiencyModelIndicating2023,
  title = {A High-Efficiency Model Indicating the Role of Inhibition in the Resilience of Neuronal Networks to Damage Resulting from Traumatic Injury},
  author = {Frost, Brian L. and Mintchev, Stanislav M.},
  year = 2023,
  month = nov,
  journal = {Journal of Computational Neuroscience},
  volume = {51},
  number = {4},
  pages = {463--474},
  issn = {1573-6873},
  doi = {10.1007/s10827-023-00860-0},
  urldate = {2025-09-04},
  abstract = {Recent investigations of traumatic brain injuries have shown that these injuries can result in conformational changes at the level of individual neurons in the cerebral cortex. Focal axonal swelling is one consequence of such injuries and leads to a variable width along the cell axon. Simulations of the electrical properties of axons impacted in such a way show that this damage may have a nonlinear deleterious effect on spike-encoded signal transmission. The computational cost of these simulations complicates the investigation of the effects of such damage at a network level. We have developed an efficient algorithm that faithfully reproduces the spike train filtering properties seen in physical simulations. We use this algorithm to explore the impact of focal axonal swelling on small networks of integrate and fire neurons. We explore also the effects of architecture modifications to networks impacted in this manner. In all tested networks, our results indicate that the addition of presynaptic inhibitory neurons either increases or leaves unchanged the fidelity, in terms of bandwidth, of the network's processing properties with respect to this damage.},
  langid = {english}
}

@article{gerstnerEligibilityTracesPlasticity2018,
  title = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}: {{Experimental Support}} of {{NeoHebbian Three-Factor Learning Rules}}},
  shorttitle = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}},
  author = {Gerstner, Wulfram and Lehmann, Marco and Liakoni, Vasiliki and Corneil, Dane and Brea, Johanni},
  year = 2018,
  month = jul,
  journal = {Frontiers in Neural Circuits},
  volume = {12},
  pages = {53},
  issn = {1662-5110},
  doi = {10.3389/fncir.2018.00053},
  urldate = {2026-01-11},
  abstract = {Most elementary behaviors such as moving the arm to grasp an object or walking into the next room to explore a museum evolve on the time scale of seconds; in contrast, neuronal action potentials occur on the time scale of a few milliseconds. Learning rules of the brain must therefore bridge the gap between these two different time scales. Modern theories of synaptic plasticity have postulated that the co-activation of pre- and postsynaptic neurons sets a flag at the synapse, called an eligibility trace, that leads to a weight change only if an additional factor is present while the flag is set. This third factor, signaling reward, punishment, surprise, or novelty, could be implemented by the phasic activity of neuromodulators or specific neuronal inputs signaling special events. While the theoretical framework has been developed over the last decades, experimental evidence in support of eligibility traces on the time scale of seconds has been collected only during the last few years. Here we review, in the context of three-factor rules of synaptic plasticity, four key experiments that support the role of synaptic eligibility traces in combination with a third factor as a biological implementation of neoHebbian three-factor learning rules.},
  langid = {english},
  keywords = {Active,Important},
  file = {C:\Users\jaeho\Zotero\storage\BIG97IL9\Gerstner et al. - 2018 - Eligibility Traces and Plasticity on Behavioral Time Scales Experimental Support of NeoHebbian Thre.pdf}
}

@book{gerstnerNeuronalDynamicsSingle2014,
  title = {Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition},
  shorttitle = {Neuronal Dynamics},
  editor = {Gerstner, Wulfram},
  year = 2014,
  publisher = {Cambridge Univ. Press},
  address = {S.l.},
  doi = {10.1017/CBO9781107447615},
  isbn = {978-1-107-63519-7 978-1-107-06083-8},
  langid = {english},
  keywords = {Important},
  file = {C\:\\Users\\jaeho\\Zotero\\storage\\WEEFXFDN\\Gerstner et al. - 2014 - Neuronal Dynamics From Single Neurons to Networks and Models of Cognition.pdf;C\:\\Users\\jaeho\\Zotero\\storage\\J85Q7DTL\\index.html}
}

@article{haoBiologicallyPlausibleSupervised2020,
  title = {A {{Biologically Plausible Supervised Learning Method}} for {{Spiking Neural Networks Using}} the {{Symmetric STDP Rule}}},
  author = {Hao, Yunzhe and Huang, Xuhui and Dong, Meng and Xu, Bo},
  year = 2020,
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  eprint = {1812.06574},
  primaryclass = {cs},
  pages = {387--395},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.007},
  urldate = {2025-11-20},
  abstract = {Spiking neural networks (SNNs) possess energy-efficient potential due to event-based computation. However, supervised training of SNNs remains a challenge as spike activities are non-differentiable. Previous SNNs training methods can be generally categorized into two basic classes, i.e., backpropagation-like training methods and plasticity-based learning methods. The former methods are dependent on energy-inefficient real-valued computation and non-local transmission, as also required in artificial neural networks (ANNs), whereas the latter are either considered to be biologically implausible or exhibit poor performance. Hence, biologically plausible (bio-plausible) high-performance supervised learning (SL) methods for SNNs remain deficient. In this paper, we proposed a novel bio-plausible SNN model for SL based on the symmetric spike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic plasticity of the dynamic threshold, our SNN model implemented SL well and achieved good performance in the benchmark recognition task (MNIST dataset). To reveal the underlying mechanism of our SL model, we visualized both layer-based activities and synaptic weights using the t-distributed stochastic neighbor embedding (t-SNE) method after training and found that they were well clustered, thereby demonstrating excellent classification ability. Furthermore, to verify the robustness of our model, we trained it on another more realistic dataset (Fashion-MNIST), which also showed good performance. As the learning rules were bio-plausible and based purely on local spike events, our model could be easily applied to neuromorphic hardware for online training and may be helpful for understanding SL information processing at the synaptic level in biological neural systems.},
  archiveprefix = {arXiv},
  keywords = {Bio-Plausible,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,MNIST,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\jaeho\\Zotero\\storage\\5L39UC9E\\Hao et al. - 2020 - A Biologically Plausible Supervised Learning Method for Spiking Neural Networks Using the Symmetric.pdf;C\:\\Users\\jaeho\\Zotero\\storage\\NQF2X3ZS\\1812.html}
}

@article{hodgkinQuantitativeDescriptionMembrane1952,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  year = 1952,
  journal = {The Journal of Physiology},
  volume = {117},
  number = {4},
  pages = {500--544},
  issn = {1469-7793},
  doi = {10.1113/jphysiol.1952.sp004764},
  urldate = {2026-02-11},
  copyright = {\copyright{} 1952 The Physiological Society},
  langid = {english},
  keywords = {Important},
  file = {C:\Users\jaeho\Zotero\storage\FAI6D28P\jphysiol.1952.html}
}

@book{izikevicDynamicalSystemsNeuroscience2014,
  title = {Dynamical {{Systems}} in {{Neuroscience}}: {{The Geometry}} of {{Excitability}} and {{Bursting}}},
  shorttitle = {Dynamical {{Systems}} in {{Neuroscience}}},
  author = {I{\v z}ikevi{\v c}, Eugene M.},
  year = 2014,
  series = {Computational {{Neuroscience Ser}}},
  publisher = {MIT Press},
  address = {Cambridge},
  collaborator = {Poggio, Tomaso A. and Sejnowski, Terrence J.},
  isbn = {978-0-262-27607-8},
  langid = {english},
  keywords = {Active},
  file = {C:\Users\jaeho\Zotero\storage\G5UBDIEK\Izhikevich - 2006 - Dynamical Systems in Neuroscience The Geometry of Excitability and Bursting.pdf}
}

@misc{jonesCanSingleNeurons2020,
  title = {Can {{Single Neurons Solve MNIST}}? {{The Computational Power}} of {{Biological Dendritic Trees}}},
  shorttitle = {Can {{Single Neurons Solve MNIST}}?},
  author = {Jones, Ilenna Simone and Kording, Konrad Paul},
  year = 2020,
  month = sep,
  number = {arXiv:2009.01269},
  eprint = {2009.01269},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.01269},
  urldate = {2025-11-20},
  abstract = {Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. This is in stark contrast to units in artificial neural networks that are generally linear apart from an output nonlinearity. If dendritic trees can be nonlinear, biological neurons may have far more computational power than their artificial counterparts. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We find that such dendrites can readily solve machine learning problems, such as MNIST or CIFAR-10, and that they benefit from having the same input onto several branches of the dendritic tree. This dendrite model is a special case of sparse network. This work suggests that popular neuron models may severely underestimate the computational power enabled by the biological fact of nonlinear dendrites and multiple synapses per pair of neurons. The next generation of artificial neural networks may significantly benefit from these biologically inspired dendritic architectures.},
  archiveprefix = {arXiv},
  keywords = {Bio-Plausible,MNIST,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\jaeho\\Zotero\\storage\\7BH87KNA\\Jones and Kording - 2020 - Can Single Neurons Solve MNIST The Computational Power of Biological Dendritic Trees.pdf;C\:\\Users\\jaeho\\Zotero\\storage\\I9MYLCPU\\2009.html}
}

@article{moldwinGradientClusteronModel2021,
  title = {The Gradient Clusteron: {{A}} Model Neuron That Learns to Solve Classification Tasks via Dendritic Nonlinearities, Structural Plasticity, and Gradient Descent},
  shorttitle = {The Gradient Clusteron},
  author = {Moldwin, Toviah and Kalmenson, Menachem and Segev, Idan},
  year = 2021,
  month = may,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {5},
  pages = {e1009015},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009015},
  urldate = {2025-11-20},
  abstract = {Synaptic clustering on neuronal dendrites has been hypothesized to play an important role in implementing pattern recognition. Neighboring synapses on a dendritic branch can interact in a synergistic, cooperative manner via nonlinear voltage-dependent mechanisms, such as NMDA receptors. Inspired by the NMDA receptor, the single-branch clusteron learning algorithm takes advantage of location-dependent multiplicative nonlinearities to solve classification tasks by randomly shuffling the locations of ``under-performing'' synapses on a model dendrite during learning (``structural plasticity''), eventually resulting in synapses with correlated activity being placed next to each other on the dendrite. We propose an alternative model, the gradient clusteron, or G-clusteron, which uses an analytically-derived gradient descent rule where synapses are "attracted to" or "repelled from" each other in an input- and location-dependent manner. We demonstrate the classification ability of this algorithm by testing it on the MNIST handwritten digit dataset and show that, when using a softmax activation function, the accuracy of the G-clusteron on the all-versus-all MNIST task (\textasciitilde 85\%) approaches that of logistic regression (\textasciitilde 93\%). In addition to the location update rule, we also derive a learning rule for the synaptic weights of the G-clusteron (``functional plasticity'') and show that a G-clusteron that utilizes the weight update rule can achieve \textasciitilde 89\% accuracy on the MNIST task. We also show that a G-clusteron with both the weight and location update rules can learn to solve the XOR problem from arbitrary initial conditions.},
  langid = {english},
  keywords = {Artificial neural networks,Bio-Plausible,Dendritic structure,Learning,MNIST,Neuronal dendrites,Neuronal plasticity,Neurons,Synapses,Synaptic plasticity},
  file = {C:\Users\jaeho\Zotero\storage\827KEV4L\Moldwin et al. - 2021 - The gradient clusteron A model neuron that learns to solve classification tasks via dendritic nonli.pdf}
}

@article{ockerSelfOrganizationMicrocircuitsNetworks2015,
  title = {Self-{{Organization}} of {{Microcircuits}} in {{Networks}} of {{Spiking Neurons}} with {{Plastic Synapses}}},
  author = {Ocker, Gabriel Koch and {Litwin-Kumar}, Ashok and Doiron, Brent},
  year = 2015,
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {8},
  pages = {e1004458},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004458},
  urldate = {2026-02-18},
  abstract = {The synaptic connectivity of cortical networks features an overrepresentation of certain wiring motifs compared to simple random-network models. This structure is shaped, in part, by synaptic plasticity that promotes or suppresses connections between neurons depending on their joint spiking activity. Frequently, theoretical studies focus on how feedforward inputs drive plasticity to create this network structure. We study the complementary scenario of self-organized structure in a recurrent network, with spike timing-dependent plasticity driven by spontaneous dynamics. We develop a self-consistent theory for the evolution of network structure by combining fast spiking covariance with a slow evolution of synaptic weights. Through a finite-size expansion of network dynamics we obtain a low-dimensional set of nonlinear differential equations for the evolution of two-synapse connectivity motifs. With this theory in hand, we explore how the form of the plasticity rule drives the evolution of microcircuits in cortical networks. When potentiation and depression are in approximate balance, synaptic dynamics depend on weighted divergent, convergent, and chain motifs. For additive, Hebbian STDP these motif interactions create instabilities in synaptic dynamics that either promote or suppress the initial network structure. Our work provides a consistent theoretical framework for studying how spiking activity in recurrent networks interacts with synaptic plasticity to determine network structure.},
  langid = {english},
  keywords = {Action potentials,Covariance,Network motifs,Neural networks,Neuronal plasticity,Neurons,Synapses,Synaptic plasticity},
  file = {C:\Users\jaeho\Zotero\storage\Y3EED6LC\Ocker et al. - 2015 - Self-Organization of Microcircuits in Networks of Spiking Neurons with Plastic Synapses.pdf}
}

@misc{paulSurveyLearningModels2024,
  title = {A Survey on Learning Models of Spiking Neural Membrane Systems and Spiking Neural Networks},
  author = {Paul, Prithwineel and Sosik, Petr and Ciencialova, Lucie},
  year = 2024,
  month = mar,
  number = {arXiv:2403.18609},
  eprint = {2403.18609},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.18609},
  urldate = {2025-11-20},
  abstract = {Spiking neural networks (SNN) are a biologically inspired model of neural networks with certain brain-like properties. In the past few decades, this model has received increasing attention in computer science community, owing also to the successful phenomenon of deep learning. In SNN, communication between neurons takes place through the spikes and spike trains. This differentiates these models from the ``standard'' artificial neural networks (ANN) where the frequency of spikes is replaced by real-valued signals. Spiking neural P systems (SNPS) can be considered a branch of SNN based more on the principles of formal automata, with many variants developed within the framework of the membrane computing theory. In this paper, we first briefly compare structure and function, advantages and drawbacks of SNN and SNPS. A key part of the article is a survey of recent results and applications of machine learning and deep learning models of both SNN and SNPS formalisms.},
  archiveprefix = {arXiv},
  keywords = {Bio-Plausible,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,MNIST},
  file = {C\:\\Users\\jaeho\\Zotero\\storage\\EXGR65ZJ\\Paul et al. - 2024 - A survey on learning models of spiking neural membrane systems and spiking neural networks.pdf;C\:\\Users\\jaeho\\Zotero\\storage\\VQFJC6J5\\2403.html}
}

@article{sadegh-zadehAdvancingNeuralComputation2024,
  title = {Advancing Neural Computation: Experimental Validation and Optimization of Dendritic Learning in Feedforward Tree Networks},
  shorttitle = {Advancing Neural Computation},
  author = {{Sadegh-Zadeh}, Seyed-Ali},
  year = 2024,
  journal = {American Journal of Neurodegenerative Disease},
  volume = {13},
  number = {5},
  pages = {49--69},
  issn = {2165591X},
  doi = {10.62347/FIQW7087},
  urldate = {2025-11-20},
  abstract = {Objectives: This study aims to explore the capabilities of dendritic learning within feedforward tree networks (FFTN) in comparison to traditional synaptic plasticity models, particularly in the context of digit recognition tasks using the MNIST dataset. Methods: We employed FFTNs with nonlinear dendritic segment amplification and Hebbian learning rules to enhance computational efficiency. The MNIST dataset, consisting of 70,000 images of handwritten digits, was used for training and testing. Key performance metrics, including accuracy, precision, recall, and F1-score, were analysed. Results: The dendritic models significantly outperformed synaptic plasticity-based models across all metrics. Specifically, the dendritic learning framework achieved a test accuracy of 91\%, compared to 88\% for synaptic models, demonstrating superior performance in digit classification. Conclusions: Dendritic learning offers a more powerful computational framework by closely mimicking biological neural processes, providing enhanced learning efficiency and scalability. These findings have important implications for advancing both artificial intelligence systems and computational neuroscience.},
  langid = {english},
  keywords = {Bio-Plausible,MNIST},
  file = {C:\Users\jaeho\Zotero\storage\3MWFJ4R9\Sadegh-Zadeh - 2024 - Advancing neural computation experimental validation and optimization of dendritic learning in feed.pdf}
}

@article{skaarSimplifiedModelNMDAreceptormediated2025,
  title = {A Simplified Model of {{NMDA-receptor-mediated}} Dynamics in Leaky Integrate-and-Fire Neurons},
  author = {Skaar, Jan-Eirik Welle and Haug, Nicolai and Plesser, Hans Ekkehard},
  year = 2025,
  month = sep,
  journal = {Journal of Computational Neuroscience},
  volume = {53},
  number = {3},
  pages = {475--487},
  issn = {1573-6873},
  doi = {10.1007/s10827-025-00911-8},
  urldate = {2025-12-15},
  abstract = {A model for NMDA-receptor-mediated synaptic currents in leaky integrate-and-fire neurons, first proposed by Wang (J Neurosci, 1999), has been widely studied in computational neuroscience. The model features a fast rise in the NMDA conductance upon spikes in a pre-synaptic neuron followed by a slow decay. In a general implementation of this model which allows for arbitrary network connectivity and delay distributions, the summed NMDA current from all neurons in a pre-synaptic population cannot be simulated in aggregated form. Simulating each synapse separately is prohibitively slow for all but small networks, which has largely limited the use of the model to fully connected networks with identical delays, for which an efficient simulation scheme exists. We propose an approximation to the original model that can be efficiently simulated for arbitrary network connectivity and delay distributions. Our results demonstrate that the approximation incurs minimal error and preserves network dynamics. We further use the approximate model to explore binary decision making in sparsely coupled networks.},
  langid = {english},
  keywords = {reduction},
  file = {C:\Users\jaeho\Zotero\storage\V6936BYM\Skaar et al. - 2025 - A simplified model of NMDA-receptor-mediated dynamics in leaky integrate-and-fire neurons.pdf}
}

@book{strikwerdaFiniteDifferenceSchemes2004,
  title = {Finite Difference Schemes and Partial Differential Equations},
  author = {Strikwerda, John C.},
  year = 2004,
  series = {Other Titles in Applied Mathematics},
  edition = {2nd ed},
  number = {88},
  publisher = {{Society for Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104)}},
  address = {Philadelphia, Pa},
  doi = {10.1137/1.9780898717938},
  abstract = {This book provides a unified and accessible introduction to the basic theory of finite difference schemes applied to the numerical solution of partial differential equations. Its objective is to clearly present the basic methods necessary to perform finite difference schemes and to understand the theory underlying the schemes},
  collaborator = {{Society for Industrial and Applied Mathematics}},
  isbn = {978-0-89871-567-5 978-0-89871-639-9 978-0-89871-793-8},
  langid = {english},
  file = {C:\Users\jaeho\Zotero\storage\TUUE85PK\Strikwerda - 2004 - Finite difference schemes and partial differential equations.pdf}
}

@article{tavanaeiMinimalSpikingNeural2015,
  title = {A {{Minimal Spiking Neural Network}} to {{Rapidly Train}} and {{Classify Handwritten Digits}} in {{Binary}} and 10-{{Digit Tasks}}},
  author = {Tavanaei, Amirhossein and Maida, Anthony S.},
  year = 2015,
  month = jul,
  journal = {International Journal of Advanced Research in Artificial Intelligence (ijarai)},
  volume = {4},
  number = {7},
  publisher = {{The Science and Information (SAI) Organization Limited}},
  issn = {2165-4069},
  doi = {10.14569/IJARAI.2015.040701},
  urldate = {2025-11-20},
  abstract = {This paper reports the results of experiments to develop a minimal neural network for pattern classification. The network uses biologically plausible neural and learning mechanisms and is applied to a subset of the MNIST dataset of handwritten digits. The research goal is to assess the classification power of a very simple biologically motivated mechanism. The network architecture is primarily a feedforward spiking neural network (SNN) composed of Izhikevich regular spiking (RS) neurons and conductance-based synapses. The weights are trained with the spike timing-dependent plasticity (STDP) learning rule. The proposed SNN architecture contains three neuron layers which are connected by both static and adaptive synapses. Visual input signals are processed by the first layer to generate input spike trains. The second and third layers contribute to spike train segmentation and STDP learning, respectively. The network is evaluated by classification accuracy on the handwritten digit images from the MNIST dataset. The simulation results show that although the proposed SNN is trained quickly without error-feedbacks in a few number of iterations, it results in desirable performance (97.6\%) in the binary classification (0 and 1). In addition, the proposed SNN gives acceptable recognition accuracy in 10-digit (0-9) classification in comparison with statistical methods such as support vector machine (SVM) and multi-perceptron neural network.},
  langid = {english},
  keywords = {Bio-Plausible,MNIST},
  file = {C:\Users\jaeho\Zotero\storage\ERYEI2EZ\Tavanaei and Maida - 2015 - A Minimal Spiking Neural Network to Rapidly Train and Classify Handwritten Digits in Binary and 10-D.pdf}
}

@article{zhengTemporalDendriticHeterogeneity2024,
  title = {Temporal Dendritic Heterogeneity Incorporated with Spiking Neural Networks for Learning Multi-Timescale Dynamics},
  author = {Zheng, Hanle and Zheng, Zhong and Hu, Rui and Xiao, Bo and Wu, Yujie and Yu, Fangwen and Liu, Xue and Li, Guoqi and Deng, Lei},
  year = 2024,
  month = jan,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {277},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-44614-z},
  urldate = {2025-11-20},
  abstract = {It is widely believed the brain-inspired spiking neural networks have the capability of processing temporal information owing to their dynamic attributes. However, how to understand what kind of mechanisms contributing to the learning ability and exploit the rich dynamic properties of spiking neural networks to satisfactorily solve complex temporal computing tasks in practice still remains to be explored. In this article, we identify the importance of capturing the multi-timescale components, based on which a multi-compartment spiking neural model with temporal dendritic heterogeneity, is proposed. The model enables multi-timescale dynamics by automatically learning heterogeneous timing factors on different dendritic branches. Two breakthroughs are made through extensive experiments: the working mechanism of the proposed model is revealed via an elaborated temporal spiking XOR problem to analyze the temporal feature integration at different levels; comprehensive performance benefits of the model over ordinary spiking neural networks are achieved on several temporal computing benchmarks for speech recognition, visual recognition, electroencephalogram signal recognition, and robot place recognition, which shows the best-reported accuracy and model compactness, promising robustness and generalization, and high execution efficiency on neuromorphic hardware. This work moves neuromorphic computing a significant step toward real-world applications by appropriately exploiting biological observations.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Bio-Plausible,Engineering,Mathematics and computing,MNIST},
  file = {C:\Users\jaeho\Zotero\storage\QVKBRPGL\Zheng et al. - 2024 - Temporal dendritic heterogeneity incorporated with spiking neural networks for learning multi-timesc.pdf}
}
